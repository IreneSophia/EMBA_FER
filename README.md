# EMBA: using the Bayesian Brain to investigate the specificity of emotion recognition differences in autism and ADHD

## Facial emotion recognition (FER)

Mutual social interactions require people to be aware of the affective state of their counterpart. An important source for this is facial expressions, which can be indicative of the emotions experienced by the other person. Individuals with autism spectrum disorder (ASD) often have difficulties with this function. Despite the extensive documentation of such differences, it is still unclear which processes underlie attenuated emotion recognition in ASD. In this project, we aim to use a prominent human brain theory called the Bayesian Brain to evaluate the impact of three mechanisms on emotion recognition in individuals with ASD at the neural and behavioural levels: (1) emotional face processing, (2) learning of associations between contextual cues and facial expressions associated with emotions, and (3) biased attention for faces. We also plan to include individuals with attention deficit hyperactivity disorder (ADHD) as clinical controls in addition to a sample of people with no neurodevelopmental disorder (NND). This allows us to determine whether differences in emotion recognition can be attributed to attentional deficits or are unspecific for the included developmental disorders. The results of this project will not only shed more light on the causes of deficits in emotion recognition in people with ASD, but also provide the basis for developing a model of the similarities and differences in processes of the Bayesian Brain in neurodevelopmental disorders.

In this repository, we will focus on the paradigm measuring facial emotion recognition (FER) compared to facial species recognition (FSR). For both tasks, we have created 5 s long videos by morphing an emotionally neutral face with a target face. In the case of the FER task, this target face is the same person expressing one of four emotions (anger, fear, happiness, sadness), while in the FSR task the target face belongs to one of four species (apes, cats, dogs, lions). These videos show an emotionally neutral human face gradually and continuously changing into a human face expressing an emotion (FER) or into an emotionally neutral face of another species (FSR). The participantsâ€™ task is to stop the video as soon as they have recognised the emotion or species. Then, they have to choose the correct emotion or species out of the four options. They are not presented with the final frame, therefore, the amount of information they have for their decision depends on when they stop the video. The remaining part of the video that participants did not need to see to correctly recognise the emotion or species captures their discrimination sensitivity. Using the FSR task in addition to the FER task allows us to determine whether differences between groups are due to facial emotion recognition specifically . We will compare three groups (ADHD vs. ASD vs. no neurodevelopmental disorder, NND) and use an eye tracker to measure gaze patterns.

Participants also perform three additional paradigms: a dot-probe task to measure face attention bias (FAB), a probabilistic learning paradigm (PAL) and a visual mismatch task (VMM). The preregistrations for this project are on [OSF](https://osf.io/znrht) and currently embargoed as the data collection is still ongoing. The preregistrations will be made public when manuscripts are submitted. 

This repository is a work in progress. The script are continuously augmented.
