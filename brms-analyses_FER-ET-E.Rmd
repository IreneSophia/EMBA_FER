---
title: "S3: ET analysis with brms - explorative"
author: "I. S. Plank"
date: "`r Sys.Date()`"
output: pdf_document
---
  
```{r settings, include=FALSE}

knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.align = 'center', fig.width = 9)
ls.packages = c("knitr",            # kable
   "ggplot2",          # plots
   "brms",# Bayesian lmms
   "designr",          # simLMM
   "bridgesampling",   # bridge_sampler
   "tidyverse",        # tibble stuff
   "ggpubr",           # ggarrange
   "ggrain",           # geom_rain
   "bayesplot",        # plots for posterior predictive checks
   "SBC", # plots for checking computational faithfulness
   "rstatix",          # anova
   "BayesFactor",
   "tidybayes",
   "ggdist"
)

lapply(ls.packages, library, character.only=TRUE)

# set cores
options(mc.cores = parallel::detectCores())

# graph settings 
c_light = "#a9afb2"; c_light_highlight = "#8ea5b2"; c_mid = "#6b98b2" 
c_mid_highlight = "#3585b2"; c_dark = "#0072b2"; c_dark_highlight = "#0058b2" 
c_green = "#009E73"
sz = 1

# custom colour palette
custom.col = c("#0072B2", "#009E73", "#CC79A7", "#D55E00")

# settings for the SBC package
use_cmdstanr = getOption("SBC.vignettes_cmdstanr", TRUE) # Set to false to use rst
options(brms.backend = "cmdstanr")
cache_dir = "./_brms_SBC_cache"
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}

# Using parallel processing
library(future)
plan(multisession)

# load the function to perform the sensitivity analysis
source('helpers/fun_bf-sens.R')

```

<style type="text/css">
  .main-container {
    max-width: 1100px;
    margin-left: auto;
    margin-right: auto;
  }
</style>

# S3.1 Introduction
  
This R Markdown script exploratively analyses eye tracking data from the FER (facial emotion recognition) task of the EMBA project. The data was preprocessed before being read into this script, specifically fixations were extracted and then combined with information on specific areas of interest: eyes, forehead, mouth and nose. SBCs were run with three groups and have also not been rerun with four groups. We assume that the results of the SBC will apply to this extension of the model as the formula, data per cell and priors stay the same.

## Some general settings

```{r set}

# number of simulations
nsim = 500

# set number of iterations and warmup for models
iter = 3000
warm = 1000

# set seed
set.seed(2468)

```


## Package versions
  
```{r lib_versions, echo=F}

print(R.Version()$version.string)

for (package in ls.packages) {
  print(sprintf("%s version %s", package, packageVersion(package)))
}

```

## Preparation

First, we load the data and combine it with demographic information including the diagnostic status of the subjects. Then, all predictors are set to sum contrasts. 

```{r prep}

# load the data created with brm-analyses_FER.Rmd
load("FER_data.RData")

# set and print the contrasts
contrasts(df.last$emo) = contr.sum(4)
contrasts(df.last$emo)
contrasts(df.last$diagnosis) = contr.sum(4)
contrasts(df.last$diagnosis)
contrasts(df.first$diagnosis) = contr.sum(4)
contrasts(df.first$diagnosis)

```

# S3.2 AOI of first fixation

We want to explore influences on the first fixation on an AOI (eyes, forehead, nose or mouth) of the stimuli. 

## Explore time to first fixation as possible covariate

```{r time_first}

# aggregate per subject, rank transform to achieve normal distribution
df.first.agg = df.first %>%
  group_by(subID, diagnosis) %>%
  summarise(
    pic_start = median(pic_start)
  ) %>%
  ungroup() %>%
  mutate(
    rpic_start = rank(pic_start)
  )

# check normal distribution
kable(df.first.agg %>% 
        group_by(diagnosis) %>%
        shapiro_test(pic_start, rpic_start) %>%
        mutate(
          sig = if_else(p < 0.05, "*", "")
        )
      )

# Bayesian ANOVA
aov.time = anovaBF(rpic_start   ~ diagnosis, data = df.first.agg)
aov.time@bayesFactor

# print some summaries
kable(df.first.agg %>%
  group_by(diagnosis) %>%
  summarise(
    first.mean = mean(pic_start),
    first.se   = sd(pic_start)/sqrt(n())
  ))

```

The log BF for the ANOVA of the start of the first fixation is `r sprintf("%.3f", aov.time@bayesFactor[["bf"]])`, meaning there is `r effectsize::interpret_bf(aov.time@bayesFactor[["bf"]], log = T)` a difference between the groups. Therefore, we can assume that the time to first fixation was similar across diagnostic groups. 

## Setting up and assessment of the model

For each trial, we have the first AOI on which the participant fixated. We include diagnosis on the population-level as well as intercept for each subject and intercept and slope for diagnosis for each video. 

```{r model_first}

code = "FER_first"

# set formula
f.first = brms::bf(AOI.code ~ diagnosis + (1 | subID) + (diagnosis | video))

# set weakly informative priors: fixation cross on nose, so others less likely
priors = c(
  prior(normal(-2, 1.5), class  = "Intercept", dpar = "mu2"),
  prior(normal(-2, 1.5), class  = "Intercept", dpar = "mu3"),
  prior(normal(-2, 1.5), class  = "Intercept", dpar = "mu4"),
  prior(normal(0,  1),   class  = "b", dpar = "mu2"),
  prior(normal(0,  1),   class  = "b", dpar = "mu3"),
  prior(normal(0,  1),   class  = "b", dpar = "mu4"),
  prior(normal(0,  1),   class  = "sd", dpar = "mu2"),
  prior(normal(0,  1),   class  = "sd", dpar = "mu3"),
  prior(normal(0,  1),   class  = "sd", dpar = "mu4"),
  prior(lkj(2),          class = cor)
)

```

```{r priorpc_SBC_first}

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # create the data and the results
  gen  = SBC_generator_brms(f.first, data = df.first, prior = priors,  family = categorical,
                            thin = 50, warmup = 20000, refresh = 2000)
  dat = generate_datasets(gen, nsim) 
  saveRDS(dat, file = file.path(cache_dir, paste0("dat_", code, ".rds")))
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                        inits = 0.1, warmup = warm, iter = iter)
  res = compute_SBC(dat, bck,
       cache_mode     = "results", 
       cache_location = file.path(cache_dir, paste0("res_", code)))
  saveRDS(res$stats, 
          file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(res$backend_diagnostics, 
          file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

We start by investigating the Rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r max(df.results$sim_id)` simulations had at least one parameter that had an rhat of at least 1.05. Additionally, `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = mean(n_divergent)))`).

Next, we can plot the simulated values to perform prior predictive checks as well as check the results of our SBC.

```{r priorpc_SBC_first2, fig.height=18}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.first)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}

truePars = dat[['variables']]

# compute one histogram per simulated data-set 
binwidth = 1 
breaks = seq(0, max(dvfakemat, na.rm=T), binwidth) 
histmat = matrix(NA, ncol = nrow(truePars), nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakemat[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs)
}
quantmat$x = c("nose", "forehead", "eyes", "mouth")
pf0 = ggplot(data = quantmat, aes(x = x)) + 
  geom_bar(aes(y = p0.9), fill = c_light, stat = "identity") + 
  geom_bar(aes(y = p0.8), fill = c_light_highlight, stat = "identity") + 
  geom_bar(aes(y = p0.7), fill = c_mid, stat = "identity") + 
  geom_bar(aes(y = p0.6), fill = c_mid_highlight, stat = "identity") + 
  geom_bar(aes(y = p0.5), fill = c_dark, stat = "identity") + 
  labs(title = "Prior predictive distribution", y = "", x = "") +
  theme_bw()

# get simulation numbers with issues
check = merge(df.results %>%
                mutate(correct_max_rank = max(max_rank)) %>%
                group_by(sim_id, correct_max_rank, max_rank) %>% 
                summarise(rhat = max(rhat, na.rm = T)) %>% 
                filter(rhat >= 1.05 | max_rank < correct_max_rank), 
              df.backend %>% filter(n_divergent > 0), all = T)
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))

# plot SBC with functions from the SBC package
p1 = plot_ecdf_diff(df.results.b) +
  theme_bw() + theme(legend.position = "none")
p2 = plot_rank_hist(df.results.b, bins = 20) +
  theme_bw()
p3 = plot_sim_estimated(df.results.b, alpha = 0.5) +
  theme_bw()
p4 = plot_contraction(df.results.b, 
         prior_sd = setNames(c(rep(1.5, 3), rep(1, 6)), 
   unique(df.results.b$variable))) +
  theme_bw()

p = ggarrange(pf0, 
 p1, p2, 
 p3, p4, labels = "AUTO", nrow = 5, heights = c(2, 3, 3, 3, 3))
annotate_figure(p, 
                top = text_grob("Prior predictive checks and SBC", 
                face = "bold", size = 14))


```

All looks good. 

## Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_first, message=T, fig.height=6}

# fit the model 
set.seed(2468)
m.first = brm(f.first,
              df.first, prior = priors,
              iter = iter, warmup = warm,
              backend = "cmdstanr", threads = threading(8),
              file = "m_first",
              family = "categorical"
              )
rstan::check_hmc_diagnostics(m.first$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.first) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.first)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 4)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples, and no rhats that are higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks.

```{r postpc2_first, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.first, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.first, ndraws = nsim, type = 'bars') + 
  theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(name = "", breaks = c(1, 2, 3, 4), 
                     labels = c("nose", "forehead", "eyes", "mouth"))

# distributions of means and sds compared to the real values per group
p2 = ppc_bars_grouped(as.numeric(df.first$AOI.code), post.pred, df.first$diagnosis) + 
  theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(name = "", breaks = c(1, 2, 3, 4), 
                     labels = c("nose", "forehead", "eyes", "mouth"))

p = ggarrange(p1, p2,
nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: first fixations", 
                face = "bold", size = 14))

```

The predictions based on the model capture the data very well, both overall and for the three diagnostic groups. 

## Inferences and plots

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_first, fig.height=6}

# print a summary
summary(m.first)

# plot the estimates
as_draws_df(m.first) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_mu2_COMP = - b_mu2_diagnosis1 - b_mu2_diagnosis2 - b_mu2_diagnosis3,
    b_mu3_COMP = - b_mu3_diagnosis1 - b_mu3_diagnosis2 - b_mu3_diagnosis3,
    b_mu4_COMP = - b_mu4_diagnosis1 - b_mu4_diagnosis2 - b_mu4_diagnosis3
  ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "mu2_", "forehead: "),
    coef = str_replace_all(coef, "mu3_", "eyes: "),
    coef = str_replace_all(coef, "mu4_", "mouth: "),
    coef = str_replace_all(coef, "diagnosis1", "ADHD"),
    coef = str_replace_all(coef, "diagnosis2", "ASD"),
    coef = str_replace_all(coef, "diagnosis3", "ADHD+ASD"),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(c_dark, c_light)) + theme(legend.position = "none")

```

Since this is a categorical model, the reference category was automatically set to AOI1 which corresponds to the nose and all estimates are log odds with respect to this category. This makes the interpretation of the estimates themselves a bit trickier. Therefore, we plot the estimated mean score corresponding to each diagnosis (thanks to Solomon who suggested this!) based on the model together with the actual data.

```{r plot_first, fig.height=4}

a = 0.66

# add epreds
df.first.epred = df.first %>% 
  tidyr::expand(diagnosis, AOI.code) %>%
  add_epred_draws(m.first, re_formula = NA)

# create a plot
df.first.epred %>% 
  ggplot() +
  geom_col(data = df.first %>% 
             group_by(diagnosis) %>% 
             count(AOI.code) %>% 
             mutate(probability = n / sum(n)),
           aes(x = AOI.code, y = probability, group = diagnosis, fill = diagnosis), 
           width = 0.8, position= "dodge", alpha = a, colour = "black") +
  stat_pointinterval(aes(x = .category, y = .epred, group = diagnosis, fill = diagnosis),
                     point_size = 2, position = position_dodge(width = 0.8), 
                     show.legend = FALSE) + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + ylim(0, 1) +
  theme(legend.position = "right") + 
  labs(title = "Expectations based on the model on top of real data", 
       y = "percentage") +
  scale_x_discrete(
    "AOI",
    labels = c(
      "1" = "nose",
      "2" = "forehead",
      "3" = "eyes",
      "4" = "mouth"
    )
  )

```

This plot suggests that the AOIs over the eyes and nose were more likely to be the target of the first fixation. There does not seem to be a difference between the four diagnostic groups.

# S3.3 AOI of last fixation

We also want to explore influences on the last fixation on an AOI (eyes, forehead, nose or mouth) of the stimuli, before participants actively stopped the video because they believed to have recognised the emotion. 

## Setting up and assessment of the model

For each trial, we have the last AOI on which the participant fixated. We include diagnosis, emotion and their interaction on the population-level as well as intercept and slopes for emotions for each subject and intercept and slope for diagnosis for each video. 

```{r model_last}

code = "FER_last"

# set formula
f.last = brms::bf(AOI.code ~ diagnosis * emo + (emo | subID) + (diagnosis | video))

# set weakly informative priors: less difference but still nose slightly more likely
priors = c(
  prior(normal(-1.5, 2),   class  = "Intercept", dpar = "mu2"),
  prior(normal(-1.5, 2),   class  = "Intercept", dpar = "mu3"),
  prior(normal(-1.5, 2),   class  = "Intercept", dpar = "mu4"),
  prior(normal(0,  1),   class  = "b", dpar = "mu2"),
  prior(normal(0,  1),   class  = "b", dpar = "mu3"),
  prior(normal(0,  1),   class  = "b", dpar = "mu4"),
  prior(normal(0,  1),   class  = "sd", dpar = "mu2"),
  prior(normal(0,  1),   class  = "sd", dpar = "mu3"),
  prior(normal(0,  1),   class  = "sd", dpar = "mu4"),
  prior(lkj(2),         class = cor)
)

```

```{r priorpc_SBC_last}

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # create the data and the results
  gen  = SBC_generator_brms(f.last, data = df.last, prior = priors, 
                            family = categorical,
  thin = 50, warmup = 20000, refresh = 2000)
  dat = generate_datasets(gen, nsim) 
  saveRDS(dat, file = file.path(cache_dir, paste0("dat_", code, ".rds")))
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                        inits = 0.1, warmup = warm, iter = iter)
  res = compute_SBC(dat, bck,
       cache_mode     = "results", 
       cache_location = file.path(cache_dir, paste0("res_", code)))
  df.results = res$stats
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, "_large.rds"))) 
  # also save a smaller version of the data frame to comply with github
  saveRDS(df.results %>% filter(substr(variable,1,2) != "r_"), 
          file = file.path(cache_dir, paste0("df_res_", code, ".rds"))) 
  saveRDS(res$backend_diagnostics, 
          file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

We start by investigating the Rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r max(df.results$sim_id)` simulations had at least one parameter that had an rhat of at least 1.05. Additionally, `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = mean(n_divergent)))`). This looks good and we proceed to plotting the simulated values to perform prior predictive checks as well as check the results of our SBC.

```{r priorpc_SBC_last2, fig.height=24}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.last)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}

truePars = dat[['variables']]

# compute one histogram per simulated data-set 
binwidth = 1 
breaks = seq(0, max(dvfakemat, na.rm=T), binwidth) 
histmat = matrix(NA, ncol = nrow(truePars), nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakemat[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs)
}
quantmat$x = c("nose", "forehead", "eyes", "mouth")
pl0 = ggplot(data = quantmat, aes(x = x)) + 
  geom_bar(aes(y = p0.9), fill = c_light, stat = "identity") + 
  geom_bar(aes(y = p0.8), fill = c_light_highlight, stat = "identity") + 
  geom_bar(aes(y = p0.7), fill = c_mid, stat = "identity") + 
  geom_bar(aes(y = p0.6), fill = c_mid_highlight, stat = "identity") + 
  geom_bar(aes(y = p0.5), fill = c_dark, stat = "identity") + 
  labs(title = "Prior predictive distribution", y = "", x = "") +
  theme_bw()

# get simulation numbers with issues
check = merge(df.results %>%
                mutate(correct_max_rank = max(max_rank)) %>%
                group_by(sim_id, correct_max_rank, max_rank) %>% 
                summarise(rhat = max(rhat, na.rm = T)) %>% 
                filter(rhat >= 1.05 | max_rank < correct_max_rank), 
              df.backend %>% filter(n_divergent > 0), all = T)
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))

# plot SBC with functions from the SBC package
p1 = plot_ecdf_diff(df.results.b) +
  theme_bw() + theme(legend.position = "none")
p2 = plot_rank_hist(df.results.b, bins = 20) +
  theme_bw()
p3 = plot_sim_estimated(df.results.b, alpha = 0.5) +
  theme_bw()
p4 = plot_contraction(df.results.b, 
         prior_sd = setNames(c(rep(2, 3), rep(1, length(unique(df.results.b$variable))-3)), 
   unique(df.results.b$variable))) +
  theme_bw()

p = ggarrange(pl0, 
 p1, p2, 
 p3, p4, labels = "AUTO", nrow = 5, heights = c(1, 2, 2, 2, 2))
annotate_figure(p, 
                top = text_grob("Prior predictive checks and SBC", 
                face = "bold", size = 14))


```

All looks good. 

## Posterior predictive checks

As the next step, we fit the model and check whether the chains have converged, which they seem to have. We then perform posterior predictive checks on the model using the bayesplot package.

```{r postpc_last, message=T, fig.height=14}

# fit the model
set.seed(8426)
m.last = brm(f.last,
              df.last, prior = priors,
              iter = iter, warmup = warm,
              backend = "cmdstanr", threads = threading(8),
              file = "m_last",
              family = "categorical"
              )
rstan::check_hmc_diagnostics(m.last$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.last) >= 1.01, na.rm = T)

# check the last 10000 iterations in trace plots
post.draws = as_draws_df(m.last)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 4)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples, and no rhats that are higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2_last, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.last, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.last, ndraws = nsim, type = 'bars') + 
  theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(name = "", breaks = c(1, 2, 3, 4), 
                     labels = c("nose", "forehead", "eyes", "mouth"))

# distributions of means compared to the real values per group
p2 = ppc_bars_grouped(as.numeric(df.last$AOI.code), post.pred, df.last$diagnosis) + 
  theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(name = "", breaks = c(1, 2, 3, 4), 
                     labels = c("nose", "forehead", "eyes", "mouth"))

p = ggarrange(p1, p2,
nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks: last fixations", 
                face = "bold", size = 14))

```

The predictions based on the model capture the data very well, both overall and for the three diagnostic groups. 

## Inferences and plots

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_last, fig.height=14}

# print a summary
summary(m.last)

# plot the posterior distributions:
as_draws_df(m.last) %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_mu2_COMP = - b_mu2_diagnosis1 - b_mu2_diagnosis2 - b_mu2_diagnosis3,
    b_mu3_COMP = - b_mu3_diagnosis1 - b_mu3_diagnosis2 - b_mu3_diagnosis3,
    b_mu4_COMP = - b_mu4_diagnosis1 - b_mu4_diagnosis2 - b_mu4_diagnosis3,
    b_mu2_sadness = - b_mu2_emo1 - b_mu2_emo2 - b_mu2_emo3,
    b_mu3_sadness = - b_mu3_emo1 - b_mu3_emo2 - b_mu3_emo3,
    b_mu4_sadness = - b_mu4_emo1 - b_mu4_emo2 - b_mu4_emo3,
  ) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "mu2_", "forehead: "),
    coef = str_replace_all(coef, "mu3_", "eyes: "),
    coef = str_replace_all(coef, "mu4_", "mouth: "),
    coef = str_replace_all(coef, "diagnosis1", "ADHD"),
    coef = str_replace_all(coef, "diagnosis2", "ASD"),
    coef = str_replace_all(coef, "diagnosis3", "ADHD+ASD"),
    coef = str_replace_all(coef, "emo1", "fear"),
    coef = str_replace_all(coef, "emo2", "anger"),
    coef = str_replace_all(coef, "emo3", "happiness"),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(c_dark, c_light)) + theme(legend.position = "none")

```

Again, the nose is the reference AOI for this model. We will create some plots to help us interpret these results. 

```{r plot_last, fig.height=6}

a = 0.66

# add epreds
df.last.epred = df.last %>% 
  tidyr::expand(diagnosis, emo, AOI.code) %>%
  add_epred_draws(m.last, re_formula = NA)

# create a plots
df.last.epred %>%
  mutate(
    emotion = fct_recode(emo, 
                         "fear" = "AF", 
                         "anger" = "AN", 
                         "happiness" = "HA", 
                         "sadness" = "SA")
    ) %>% 
  ggplot() +
  geom_col(data = df.last %>% 
             group_by(diagnosis, emo) %>% 
             count(AOI.code) %>% 
             mutate(probability = n / sum(n)),
           aes(x = AOI.code, 
               y = probability, 
               group = diagnosis, 
               fill = diagnosis), 
           width = 0.8, position= "dodge", alpha = a, colour = "black") +
  stat_pointinterval(aes(x = .category, y = .epred, group = diagnosis, fill = diagnosis),
                     point_size = 2, position = position_dodge(width = 0.8), 
                     show.legend = FALSE) + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + ylim(0, 1) +
  theme(legend.position = "bottom") + 
  labs(title = "Expectations based on the model on top of real data", y = "percentage") +
  facet_wrap(. ~ emo) +
  scale_x_discrete(
    "AOI",
    labels = c(
      "1" = "nose",
      "2" = "forehead",
      "3" = "eyes",
      "4" = "mouth"
    )
  )

```

Again, these plots show stark differences between the four AOIs. The last AOI to be fixated on does not seem to differ between the diagnostic groups, but it might differ between the emotions. 